{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we compare the optimal potentials obtained from different algorithms namely: Sinkhorn, damped Newton and semi-dual damped Newton with the optimal potentials obtained from using log-domain Sinkhorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(1234)\n",
    "%matplotlib inline \n",
    "%load_ext autoreload                                                                                                                                                                                                \n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path_to_new_folder = \"../Images\"\n",
    "os.makedirs(relative_path_to_new_folder, exist_ok = True)\n",
    "if not os.path.isdir('../Images/Correctness_images'):\n",
    "    os.makedirs('../Images/Correctness_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To compute distance matrix\"\"\"\n",
    "def distmat( x, y ):\n",
    "    return np.sum( x**2, 0 )[:,None] + np.sum( y**2, 0 )[None,:] - 2 * x.transpose().dot( y )\n",
    "\n",
    "\"\"\"To Normalise a vector\"\"\"\n",
    "normalize = lambda a: a/np.sum( a )\n",
    "\n",
    "\"\"\"To Compute P\"\"\"\n",
    "def GetP( u, K, v ):\n",
    "    return u[:,None] * K * v[None,:]\n",
    "\n",
    "def plotp( x, col, plt, scale = 200, edgecolors = \"k\" ):\n",
    "  return plt.scatter( x[0,:], x[1,:], s = scale, edgecolors = edgecolors, c = col, cmap = 'plasma', linewidths = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data( N ):\n",
    "    \"\"\"\n",
    "     N is a list of the size of the data on x and y\n",
    "    \"\"\"\n",
    "    x = np.random.rand( 2, N[0] ) - 0.5\n",
    "    theta = 2 * np.pi*np.random.rand( 1, N[1] )\n",
    "    r = 0.8 + .2 * np.random.rand( 1, N[1] )\n",
    "    y = np.vstack( ( r * np.cos( theta ), r * np.sin( theta ) ) )\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import computational_OT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make potentials independent of any shift by constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_potentials( f, g ):\n",
    "    # Fixes if f and g have extra useless dimensions\n",
    "    f = f.flatten()\n",
    "    g = g.flatten()\n",
    "    #\n",
    "    ones_N = np.ones_like(f)\n",
    "    ones_M = np.ones_like(g)\n",
    "    coeff = ( np.sum(f) - np.sum(g) )/( len(f) + len(g) )\n",
    "    f_new = f - coeff * ones_N\n",
    "    g_new = g + coeff * ones_M\n",
    "    return ( f_new, g_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [ 500, 600 ]\n",
    "x, y = generate_data( N )\n",
    "epsilons  = [ 1.0, 0.5, 0.1,  0.05, 0.03 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized formulation\n",
    "\n",
    "The primal entropy regularized formulation of OT is given by:\n",
    "$$\n",
    "OT_{\\varepsilon}(\\alpha,\\beta) = min_{\\pi \\in \\mathcal{U}(\\alpha,\\beta)} \\langle C,\\pi \\rangle +\\varepsilon KL(\\pi\\|\\alpha \\otimes \\beta)\\ ,\n",
    "$$\n",
    "where\n",
    "$\\ \n",
    "KL(\\pi\\|\\alpha \\otimes \\beta) \n",
    "\\ $ is the KL-divergence and $\\ \\mathcal{U}(\\alpha,\\beta)=\\{\\pi: \\pi\\mathcal{1}=\\alpha, \\pi^{T}\\mathcal{1}=\\beta\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn  \n",
    "The optimal coupling $\\pi^{*}$ has the following form :\n",
    "$$\n",
    "\\pi^{*} = \\alpha \\odot diag(u)K diag(v)\\odot \\beta\n",
    "$$\n",
    "and we know that $\\pi^{*}\\mathbb{1}=\\alpha$ and $(\\pi^{*})^{T}\\mathbb{1}=\\beta$.\n",
    "###\n",
    "Therefore, Sinkhorn updates is given by the following iterative projections\n",
    "$$\n",
    "u^{t+1}  \\leftarrow \\frac{1}{K(v^{t}\\odot \\beta)}\\ , \\\n",
    "v^{t+1}  \\leftarrow \\frac{1}{K^{T}(u^{t+1}\\odot \\alpha)}\\ , \n",
    "$$\n",
    "where \n",
    "$K = e^{-\\frac{C}{\\varepsilon}}\\in M_{n\\times m}(\\mathbb{R}),\\ \\alpha \\in \\mathbb{R}^{n},\\ \\beta \\in \\mathbb{R}^{m}\\ ,\\ u\\in \\mathbb{R}^{n},\\ v\\in \\mathbb{R}^{m}\\ and \\ (u^{0},v^{0})=(u,v)\\ .$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy regularized dual-formulation\n",
    "The dual formulation of the entropy regularized OT is given by:\n",
    "$$\n",
    "OT_{\\varepsilon}(\\alpha,\\beta) = \\max_{f\\in \\mathbb{R}^{n}, g\\in\\mathbb{R}^{m}} \\langle f, \\alpha \\rangle + \\langle g, \\beta \\rangle - \\varepsilon\\left(\\langle\\alpha \\otimes \\beta, e^{\\frac{f}{\\varepsilon}}\\odot K \\odot e^{\\frac{g}{\\varepsilon}}  \\rangle-1\\right)\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\alpha \\in \\mathcal{M}_{1}(\\mathcal{X}),\\ \\beta \\in \\mathcal{M}_{1}(\\mathcal{Y}),\\ \\varepsilon>0,\\ f\\in\\mathbb{R}^{n},\\ g\\in \\mathbb{R}^{m}\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-domain Sinkhorn\n",
    "Now, the exp-log regularized update of the Sinkhorn algorithm is as follows:\n",
    "$$\n",
    "m_{i}(g)\\leftarrow \\min_{j}(C_{ij}-g_{j}^{(t)}),\\ \\forall\\  i = 1,\\dots,n\\ ,\n",
    "$$\n",
    "$$\n",
    "f^{(t+1)}_{i}\\leftarrow -\\varepsilon \\log\\left(\\sum_{j=1}^{m}\\exp\\left(\\frac{-\\left(C_{ij}-g_{j}^{(t)}-m_{i}(g)\\right)}{\\varepsilon}\\right)\\beta_{j}\\right)+m_{i}(g),\\ \\forall\\  i=1,\\dots,n\\ ,\n",
    "$$\n",
    "$$\n",
    "m_{j}(f)\\leftarrow \\min_{i}(C_{ij}-f_{i}^{(t+1)}),\\ \\forall\\   j=1,\\dots,m\\\n",
    " ,\n",
    "$$\n",
    "$$\n",
    "g^{(t+1)}_{j}\\leftarrow -\\varepsilon \\log\\left(\\sum_{i=1}^{n}\\exp\\left(\\frac{-\\left(C_{ij}-f_{i}^{(t+1)}-m_{j}(f)\\right)}{\\varepsilon}\\right)\\alpha_{i}\\right)+m_{j}(f),\\ \\forall\\  j=1,\\dots,m\\ ,\n",
    "$$\n",
    "where \n",
    "$K=e^{-C/\\varepsilon} \\in M_{n \\times m}(\\mathbb{R}),\\ $ $\\varepsilon >0,\\ $ $\\alpha \\in \\mathbb{R}^{n},\\ $ $\\beta \\in \\mathbb{R}^{m},\\ $\n",
    "   $f \\in \\mathbb{R}^{n},\\ $ $g \\in \\mathbb{R}^{m}\\ and \\ (f^{(0)},g^{(0)})=(f,g)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Log-domain Sinkhorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log domain Sinkhorn\n",
    "print( \"Log domain Sinkhorn... \" )\n",
    "print( \"Doing for (\",N[0],N[1],\").\" )\n",
    "results_logSinkhorn = []\n",
    "times_logSinkhorn   = []\n",
    "logsinkhornP        = []\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ))\n",
    "#Cost matrix\n",
    "C = distmat( x, y )\n",
    "for eps in epsilons:\n",
    "  print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "  print( \" |- Iterating\" )\n",
    "  start = time.time()\n",
    "  logsinkhorn = computational_OT.log_domainSinkhorn_np(   a,\n",
    "                                                          b,\n",
    "                                                          C,\n",
    "                                                          eps )\n",
    "  out = logsinkhorn.update( max_iterations = 500 )\n",
    "  results_logSinkhorn.append( out )\n",
    "  end = time.time()\n",
    "  times_logSinkhorn.append( end - start ) \n",
    "  print( \" |- Computing P\" )\n",
    "  print( \"\" )\n",
    "  u_opt = np.exp( out['potential_f']/eps )\n",
    "  K = np.exp( - C/eps )\n",
    "  v_opt =  np.exp( out['potential_g']/eps )\n",
    "  P_opt = GetP( u_opt, K, v_opt )\n",
    "  logsinkhornP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len( results_logSinkhorn) ):\n",
    "  error = np.asarray( results_logSinkhorn[i]['error'])\n",
    "  plt.plot( error, label = 'Log-sinkhorn for $\\epsilon = $'+ str(epsilons[i]) , linewidth = 2 )\n",
    "# end for\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.xlabel( \"Iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_log_domain_Sinkhorn.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flogsinkhorn, glogsinkhorn = [], []\n",
    "for i in range(len(results_logSinkhorn)):\n",
    "    flogsinkhorn.append(results_logSinkhorn[i]['potential_f'])\n",
    "    glogsinkhorn.append(results_logSinkhorn[i]['potential_g'])\n",
    "# end for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Sinkhorn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn\n",
    "print( \"Sinkhorn... \" )\n",
    "print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "SinkhornP = []\n",
    "results_Sinkhorn = []\n",
    "times_Sinkhorn = []\n",
    "#Cost matrix\n",
    "C = distmat( x, y )\n",
    " # a and b\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "for eps in epsilons:\n",
    "  #Kernel\n",
    "  K = np.exp( - C/eps )\n",
    "  print( \" |- Iterating\" )\n",
    "  #Inflating\n",
    "  u = a\n",
    "  v = b\n",
    "  start = time.time()\n",
    "  Optimizer = computational_OT.sinkhorn(  K,\n",
    "                                          a,\n",
    "                                          b,\n",
    "                                          u,\n",
    "                                          v,\n",
    "                                          eps )\n",
    "  out = Optimizer._update()\n",
    "  results_Sinkhorn.append( out ) \n",
    "  end = time.time()\n",
    "  times_Sinkhorn.append( end - start )\n",
    "  print( \" |- Computing P\" )\n",
    "  print( \"\" )\n",
    "  u_opt = np.exp( out['potential_f']/eps )\n",
    "  K = np.exp( - C/eps )\n",
    "  v_opt =  np.exp( out['potential_g']/eps )\n",
    "  P_opt = GetP( u_opt, K, v_opt )\n",
    "  SinkhornP.append( P_opt )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.subplot( 2, 1, 1 ),\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range( len(results_Sinkhorn) ):\n",
    "  error = np.asarray( results_Sinkhorn[i]['error_a'] ) + np.asarray( results_Sinkhorn[i]['error_b'] )\n",
    "  plt.plot( error,label = 'Sinkhorn for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2 )\n",
    "# end for\n",
    "plt.yscale( 'log' )\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.xlabel( \"Iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_Sinkhorn.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsinkhorn, gsinkhorn = [], []\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    fsinkhorn.append(results_Sinkhorn[i]['potential_f'])\n",
    "    gsinkhorn.append(results_Sinkhorn[i]['potential_g'])\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Sinkhorn keys: \" )\n",
    "print( out.keys() )\n",
    "print( \"Sinkhorn log domain keys\" )\n",
    "print( out.keys() )\n",
    "# TODO: Make same keys\n",
    "print( \"\")\n",
    "print( fsinkhorn[0].shape )\n",
    "print( flogsinkhorn[0].shape )\n",
    "# Make outputs have same formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality checks\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    print( f'''i : {i}''') \n",
    "    # Couplings\n",
    "    P_logSK =   logsinkhornP[i]\n",
    "    P_SK    = SinkhornP[i]\n",
    "    error   = np.linalg.norm( P_SK - P_logSK, ord = 'fro' )\n",
    "    print( \"Error of couplings : \", error )\n",
    "    # Sums of potentials f_i + g_j\n",
    "    sum_SK    = fsinkhorn[i][:,None] + gsinkhorn[i][None,:]\n",
    "    sum_logSK = flogsinkhorn[i][:,None] + glogsinkhorn[i][None,:]\n",
    "    print( sum_SK.shape, sum_logSK.shape )\n",
    "    print( np.mean(sum_SK), np.mean(sum_logSK) )\n",
    "    sum_SK = sum_SK.squeeze()\n",
    "    sum_logSK = sum_logSK.squeeze()\n",
    "    print( sum_SK.shape, sum_logSK.shape )\n",
    "    error = np.linalg.norm( sum_SK - sum_logSK, ord = np.inf )\n",
    "    print( \"Error of sums of potentials : \", error )\n",
    "    print( \"\")\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the final potentials of Sinkhorn and log-domian Sinkhorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_logSK = []\n",
    "unique_SK = []\n",
    "errors_f = []\n",
    "errors_g = []\n",
    "for i in range(len(results_Sinkhorn)):\n",
    "    print( f'''i : {i}''' )\n",
    "    unique_logSK.append( make_unique_potentials( flogsinkhorn[i], glogsinkhorn[i] ) )\n",
    "    unique_SK.append( make_unique_potentials( fsinkhorn[i], gsinkhorn[i] ) )\n",
    "    print(unique_logSK[i][0].shape,unique_SK[i][0].shape)\n",
    "    err_f = np.linalg.norm( unique_logSK[-1][0] - unique_SK[-1][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[-1][1] - unique_SK[-1][1] )\n",
    "    errors_f.append( err_f )\n",
    "    errors_g.append( err_g )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"Difference between potentials.\" )\n",
    "plt.plot( list(range(len(epsilons))), np.array(errors_f[::-1]) + np.array(errors_g[::-1]), label = 'difference for potentials ( f, g ) between log-domain Sinkhorn and Sinkhorn ', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_Sinkhorn.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Damped Newton using direct inversion method without preconditioning \n",
    "Let us denote the objective function of the dual formulation by $Q_{\\alpha, \\beta,\\varepsilon}$.\n",
    "\n",
    "\n",
    "The Hessian of the dual formulation of the entropy regularized OT is given by \n",
    "$$\n",
    "\\nabla^{2}Q_{\\alpha, \\beta,\\varepsilon}(f,g)=\\frac{-1}{\\varepsilon}\n",
    "\\begin{pmatrix}\n",
    "\\Delta(\\alpha) && \\pi_{\\varepsilon}\\\\\n",
    "\\pi^{T}_{\\varepsilon} && \\Delta(\\beta) \n",
    "\\end{pmatrix}\n",
    "\\ , \n",
    "$$ \n",
    "where $\\pi\\mathbb{1}_{m} = \\alpha,\\ \\pi^{T}\\mathbb{1}_{n}=\\beta,\\ $ and $\\Delta = diag: \\mathbb{R}^{n} \\rightarrow M_{n}(\\mathbb{R})$ is the linear operator mapping a vector  to a diagonal matrix  containing  this vector.\n",
    "\n",
    "\n",
    "This implies \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\Delta(\\alpha) && \\pi_{\\varepsilon}\\\\\n",
    "\\pi^{T}_{\\varepsilon} && \\Delta(\\beta) \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\mathbb{1}_{n}\\\\\n",
    "\\mathbb{1}_{m}\n",
    "\\end{pmatrix} = 0\\ ,\n",
    "$$\n",
    "that is,\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\mathbb{1}_{n}\\\\\n",
    "\\mathbb{1}_{m}\n",
    "\\end{pmatrix}\\in \\ker(\\nabla^{2}Q_{\\alpha, \\beta,\\varepsilon}(f,g))\\ .\n",
    "$$\n",
    "Hence, $\\nabla^{2}Q_{\\alpha, \\beta,\\varepsilon}(f,g)$ is singular. Therefore, on regularization we have the following Hessian\n",
    "$\n",
    "H_{reg} := \\nabla^{2}Q_{\\alpha, \\beta,\\varepsilon}(f,g)+\\lambda cc^{T}\\ ,\n",
    "$ \n",
    "where $c= \\begin{pmatrix}\\frac{\\mathbb{1}}{\\sqrt{n+m}}\\\\-\\frac{\\mathbb{1}}{\\sqrt{n+m}}\\end{pmatrix}\\in M_{(n+m),1}(\\mathbb{R})$.\n",
    "\n",
    "Now, at the $k^{th}$ iteration solve\n",
    "$\\nabla^{2}Q_{\\alpha, \\beta,\\varepsilon}(f,g)p_{k} = \\nabla Q_{\\alpha, \\beta,\\varepsilon}(f,g)$ to obtain the optimizing direction vector $p_{k}$ and then perform the Armijo condition to obtain the update step $\\alpha_{k}$ such that we have the update\n",
    "$$\n",
    "(f,g) \\leftarrow (f,g) + \\alpha_{k} p_{k}\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damped Newton\n",
    "print(\"Damped Newton... \")\n",
    "print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "rho = 0.95\n",
    "c = 0.05\n",
    "dampedNewtonP = []\n",
    "results_dampedNewton  = []\n",
    "times_dampedNewton    = []\n",
    "Hessians_dampedNewton = []\n",
    "#Cost matrix\n",
    "C = distmat( x, y ) \n",
    "# a and b\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "for eps in epsilons:\n",
    "    print( \"For epsilon = \"+str(eps)+\":\" )   \n",
    "    #Kernel\n",
    "    K = np.exp( - C/eps )\n",
    "    f, g = a, b\n",
    "    print( \" |- Iterating\" )   \n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.damped_Newton( K,\n",
    "                                                a,\n",
    "                                                b,\n",
    "                                                f,\n",
    "                                                g,\n",
    "                                                eps,\n",
    "                                                rho,\n",
    "                                                c )\n",
    "    out = Optimizer._update(    max_iterations = 50,\n",
    "                                debug = False )\n",
    "    end = time.time()\n",
    "    if out != -1:\n",
    "        results_dampedNewton.append( out )\n",
    "        times_dampedNewton.append( end - start )\n",
    "        print( \" |- Computing P\" )\n",
    "        print( \"\" )\n",
    "        u_opt = np.exp( out['potential_f']/eps )\n",
    "        K = np.exp( - C/eps )\n",
    "        v_opt =  np.exp( out['potential_g']/eps )\n",
    "        P_opt = GetP( u_opt, K, v_opt )\n",
    "        dampedNewtonP.append( P_opt )\n",
    "        print( \" |- Recording (unstabilized) Hessian \\n\" )\n",
    "        mat  = - eps * Optimizer.Hessian\n",
    "        diag = 1/np.sqrt( np.concatenate( ( a, b ), axis = None ) )\n",
    "        mat = diag[:,None] * mat * diag[None,:]\n",
    "        Hessians_dampedNewton.append( mat )\n",
    "    else:\n",
    "        epsilons.remove( eps )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for i in range(len(results_dampedNewton)):\n",
    "  error = np.asarray( results_dampedNewton[i]['error_a'] ) + np.asarray( results_dampedNewton[i]['error_b'] )\n",
    "  plt.plot( error, label = 'Damped Newton for $\\epsilon = $'+str(epsilons[i]), linewidth = 2 )\n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_dampedNewton.pdf\", format = 'pdf' )  \n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the final potentials of log-domain Sinkhorn and damped Newton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewton, gdampednewton = [], []\n",
    "for i in range(len(results_dampedNewton)):\n",
    "    fdampednewton.append( results_dampedNewton[i]['potential_f'] )\n",
    "    gdampednewton.append( results_dampedNewton[i]['potential_g'] )\n",
    "# end for\n",
    "unique_dampednewton = []\n",
    "for i in range(len(results_dampedNewton)):\n",
    "    unique_dampednewton.append( make_unique_potentials( fdampednewton[i], gdampednewton[i] ) )\n",
    "# end for\n",
    "errors_f, errors_g = [], []\n",
    "for i in range(len(results_dampedNewton)):\n",
    "    print( f'''i : {i}''' )\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewton[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewton[i][1] )\n",
    "    errors_f.append( err_f )\n",
    "    errors_g.append( err_g )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\")\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"Difference between potentials.\" )\n",
    "plt.plot( list(range(len(epsilons))), np.array(errors_f[::-1]) + np.array(errors_g[::-1]), label = 'difference for potentials ( f, g ) between log-domain Sinkhorn and damped Newton using direct solver', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel(\"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_dampedNewton.pdf\", format = 'pdf' ) \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Damped Newton with Preconditioning\n",
    "Here we perform dual damped Newton with preconditioning. Here we consider $t$ eigenvalues of the Hessian that we want to move to one and form the following preconditioning matrix using the corresponding eigenvectors,\n",
    "$$\n",
    "P = \\left(I_{n+m}-\\sum_{i-1}^{t}\\left(1 - \\frac{1}{\\sqrt{\\lambda_{i}}}\\right)y_{i}y_{i}^{T}\\right)\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "y_{i} \\in \\ker\\left(\\nabla^{2}_{f}Q_{\\alpha, \\beta, \\varepsilon}(f)-\\lambda_{i}I_{n}\\right),\\ \\forall i= 1,\\dots,k\\ ,\n",
    "$$\n",
    " are orthonormal.\n",
    "\n",
    "Now, at the $k^{th}$ iteration we solve the following equation:\n",
    "$$\n",
    "(P\\nabla^{2}Q_{\\alpha, \\beta, \\varepsilon}(f)P)(Pp_{k})=P\\nabla Q_{\\alpha, \\beta, \\varepsilon}(f)\\ ,\n",
    "$$\n",
    "using iterative inversion methods such as \"Conjugate gradient\" and \"GMRES\" to get the ascent direction $p_{k}$, following which we use the Armijo condition to obtain the ascent step size $\\alpha_{k}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preconditioners( num_eigs, modified_Hessian, ansatz = True ):\n",
    "    # Diagonalize\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh( modified_Hessian )\n",
    "    sorting_indices = np.argsort( eigenvalues )\n",
    "    eigenvalues  = eigenvalues[ sorting_indices ]\n",
    "    eigenvectors = eigenvectors[ : , sorting_indices ]\n",
    "    # Form null vector\n",
    "    if not ansatz:\n",
    "        null_vector = eigenvectors[:, 0]\n",
    "    else:\n",
    "        null_vector = np.hstack( (np.ones(N[0]), -np.ones(N[1])) )\n",
    "        norm = np.sqrt( N[0] + N[1] )\n",
    "        null_vector = null_vector/norm\n",
    "    # Form other vectors (only 13)\n",
    "    _,m = eigenvectors.shape\n",
    "    indices=[]\n",
    "    for i in range(num_eigs//2):\n",
    "        indices.append( m - i - 1 )\n",
    "        indices.append( i + 1 )\n",
    "    # end for\n",
    "    if num_eigs//2 != 0:\n",
    "        indices.append( m - 1 - num_eigs//2 )\n",
    "    precond_vectors = eigenvectors[:, indices ]\n",
    "    precond_vectors = []\n",
    "    for index in indices:\n",
    "        precond_vectors.append( eigenvectors[:,index] )\n",
    "    # end for\n",
    "    return null_vector, precond_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigs = 13\n",
    "null_vector, precond_vectors = build_preconditioners( num_eigs, Hessians_dampedNewton[-1], ansatz = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damped Newton with preconditioning\n",
    "print(\"Damped Newton with preconditioning... \")\n",
    "print( \"Doing for (\",N[0], N[1],\").\" )\n",
    "rho = 0.95\n",
    "c = 0.05\n",
    "reset_starting_point = True\n",
    "final_modified_Hessians = []\n",
    "dampedNewtonwithprecondP = []\n",
    "results_dampedNewtonwithprecond  = []\n",
    "times_dampedNewtonwithprecond    = []\n",
    "f, g = None, None\n",
    "# Cost matrix\n",
    "C = distmat( x, y )    \n",
    "# a and b\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "for eps in epsilons:\n",
    "    print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "    #Kernel\n",
    "    K = np.exp( - C/eps )\n",
    "    if (f is None) or (g is None): \n",
    "        f, g = 0 * a, 0 * b\n",
    "    print( \"Doing for (\",N[0],N[1],\").\" )\n",
    "    print( \" |- Iterating\" )  \n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.damped_Newton_with_preconditioning(    K,\n",
    "                                                                        a,\n",
    "                                                                        b,\n",
    "                                                                        f,\n",
    "                                                                        g,\n",
    "                                                                        eps,\n",
    "                                                                        rho,\n",
    "                                                                        c,\n",
    "                                                                        null_vector,\n",
    "                                                                        precond_vectors[:] )\n",
    "    out = Optimizer._update(    max_iterations = 50,\n",
    "                                iterative_inversion = 30,\n",
    "                                version = None,\n",
    "                                debug = False,\n",
    "                                optType = 'cg' )\n",
    "    results_dampedNewtonwithprecond.append( out )\n",
    "    end = time.time()\n",
    "    times_dampedNewtonwithprecond.append( end - start  )\n",
    "    print( \" |- Computing P\" )\n",
    "    print( \"\" )\n",
    "    u_opt = np.exp( out['potential_f']/eps )\n",
    "    K = np.exp( - C/eps )\n",
    "    v_opt =  np.exp( out['potential_g']/eps )\n",
    "    P_opt = GetP( u_opt, K, v_opt )\n",
    "    dampedNewtonwithprecondP.append( P_opt )\n",
    "    if not reset_starting_point:\n",
    "        f = Optimizer.x[:a.shape[0]]\n",
    "        g = Optimizer.x[a.shape[0]:]\n",
    "    final_modified_Hessians.append( Optimizer.modified_Hessian )\n",
    "# end for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P^T 1 -b||_1$\" )\n",
    "for i in range(len(results_dampedNewtonwithprecond)):\n",
    "  error = np.asarray( results_dampedNewtonwithprecond[i]['error_a'] ) + np.asarray( results_dampedNewtonwithprecond[i]['error_b'] )\n",
    "  plt.plot( error, label = 'Damped Newton for $\\epsilon = $'+str(epsilons[i]), linewidth = 2 )\n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_dampedNewton_with_preconditioning.pdf\", format = 'pdf' ) \n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the final potentials of log-domain Sinkhorn and damped Newton with preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewtonwithprecond, gdampednewtonwithprecond = [], []\n",
    "for i in range(len(results_dampedNewtonwithprecond)):\n",
    "    fdampednewtonwithprecond.append( results_dampedNewtonwithprecond[i]['potential_f'] )\n",
    "    gdampednewtonwithprecond.append( results_dampedNewtonwithprecond[i]['potential_g'] )\n",
    "unique_dampednewtonwithprecond = []\n",
    "for i in range(len(results_dampedNewtonwithprecond)):\n",
    "    unique_dampednewtonwithprecond.append( make_unique_potentials( fdampednewtonwithprecond[i], gdampednewtonwithprecond[i] ) )\n",
    "errors_f, errors_g = [], []\n",
    "for i in range(len(results_dampedNewtonwithprecond)):\n",
    "    print( f'''i : {i}''')\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonwithprecond[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonwithprecond[i][1] )\n",
    "    errors_f.append( err_f )\n",
    "    errors_g.append( err_g )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the final potentials of damped Newton and damped Newton with preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"For damped Newton with and without precodnitioning\" )\n",
    "for i in range(len(results_dampedNewtonwithprecond)):\n",
    "    print( f'''i : {i}''' )\n",
    "    # Sums of potentials f_i + g_j\n",
    "    sum_dampedNewton    = fdampednewton[i][:,None] + gdampednewton[i][None,:]\n",
    "    sum_dampedNewtonprecond = fdampednewtonwithprecond[i][:,None] + gdampednewtonwithprecond[i][None,:]\n",
    "    sum_dampedNewton    = sum_dampedNewton.squeeze()\n",
    "    sum_dampedNewtonprecond = sum_dampedNewtonprecond.squeeze()\n",
    "    error     = np.linalg.norm( sum_dampedNewton - sum_dampedNewtonprecond, ord = np.inf )\n",
    "    print( \"Error of sums of potentials : \", error )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"Difference between potentials.\" )\n",
    "plt.plot( list(range(len(epsilons))), np.array(errors_f[::-1]) + np.array(errors_g[::-1]), label = 'difference for potentials ( f, g ) between log-domain Sinkhorn  and damped Newton with preconditioning', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_dampedNewton_with_preconditioning.pdf\", format = 'pdf')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The semi-dual formulation of OT:\n",
    "Using the Shrodinger-bridge equations between the potentials, that is, $g_{j} = -\\varepsilon\\log\\left(\\sum_{i}\\exp\\left(\\frac{f_{i}-C_{ij}}{\\varepsilon}\\right)\\alpha_{i}\\right)\\ , \\ \\forall j = 1,\\dots,m$, the dual formulation of the objective function $Q_{\\alpha, \\beta,\\varepsilon}$ reduces to the semi-dual formulation of the objective function given by,\n",
    "$$\n",
    "Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f) = \\langle f, \\alpha \\rangle + \\langle g(f,C,\\varepsilon), \\beta \\rangle\\ , \n",
    "$$\n",
    "where\n",
    "$g(f,C,\\varepsilon)_{j} = -\\varepsilon\\log\\left(\\sum_{i}\\exp\\left(\\frac{f_{i}-C_{ij}}{\\varepsilon}\\right)\\alpha_{i}\\right)$.\n",
    "\n",
    "In this setup, the gradients and the Hessian is as follows,\n",
    "\n",
    "$a)$ Gradients:\n",
    "$$\n",
    "\\nabla_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{i} = \\frac{1}{\\varepsilon}\\alpha_{i}\\left(1-\\sum_{s=1}^{n}\\frac{e^{\\frac{f_{i}-C_{ij}}{\\varepsilon}}\\beta_{s}}{\\left(\\sum_{t=1}^{n}\\alpha_{t}e^{\\frac{f_{t}-C_{ts}}{\\varepsilon}}\\right)}\\right)\\ ,\\ \\forall i = 1,\\dots,n\\ .\n",
    "$$\n",
    "\n",
    "b) Hessian:\n",
    "$$\n",
    "\\nabla^{2}_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{ii} = \\frac{-1}{\\varepsilon}\\sum_{s=1}^{m}\\left(\\alpha_{i}\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\left(1 - \\alpha_{i}\\left(\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\right)\\beta_{s}\\ ,\\ \\forall i =1,\\dots,n\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla^{2}_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{ij} = \\frac{1}{\\varepsilon}\\sum_{s=1}^{m}\\alpha_{i}\\alpha_{j}\\left(\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\left(\\exp\\left(\\frac{f_{j}+g(f,C,\\varepsilon)_{s}-C_{js}}{\\varepsilon}\\right)\\right)\\beta_{s}\\ ,\\ \\forall i \\neq j = 1,\\dots,n\\ .\n",
    "$$\n",
    "Now we plug-in these gradients and Hessian in damped Newton algorithm as we did before.\n",
    "\n",
    "Here we also incorporate the exp-log stabilization to stabilize $g$,the gradients and the Hessian in the following way,\n",
    "$$\n",
    "f^{C}_{j} \\leftarrow \\min_{i}(C_{ij}-f_{i})\\ ,  \\ \\forall j = 1,\\dots,m,\\ \\text{the C-transform of f}\\ . \\\\\n",
    "g_{j} = f^{C}_{j} -\\varepsilon\\log\\left(\\sum_{i}\\exp\\left(\\frac{f_{i}+f^{C}_{j}-C_{ij}}{\\varepsilon}\\right)\\alpha_{i}\\right)\\ ,  \\ \\forall j = 1,\\dots,m\\ ,\n",
    "$$\n",
    "$$\n",
    "\\nabla_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{i} = \\frac{1}{\\varepsilon}\\alpha_{i}\\left(1-\\sum_{s=1}^{n}\\frac{e^{\\frac{f_{i}+f^{C}_{j}-C_{ij}}{\\varepsilon}}\\beta_{s}}{\\left(\\sum_{t=1}^{n}\\alpha_{t}e^{\\frac{f_{t}+f^{C}_{j}-C_{ts}}{\\varepsilon}}\\right)}\\right)\\ ,\\ \\forall i = 1,\\dots,n\\ , \n",
    "$$\n",
    "$$\n",
    "\\nabla^{2}_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{ii} = \\frac{-1}{\\varepsilon}\\sum_{s=1}^{m}\\left(\\alpha_{i}\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\left(1 - \\alpha_{i}\\left(\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\right)\\beta_{s}\\ ,\\ \\forall i =1,\\dots,n\\ , \n",
    "$$\n",
    "$$\n",
    "\\nabla^{2}_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)_{ij} = \\frac{1}{\\varepsilon}\\sum_{s=1}^{m}\\alpha_{i}\\alpha_{j}\\left(\\exp\\left(\\frac{f_{i}+g(f,C,\\varepsilon)_{s}-C_{is}}{\\varepsilon}\\right)\\right)\\left(\\exp\\left(\\frac{f_{j}+g(f,C,\\varepsilon)_{s}-C_{js}}{\\varepsilon}\\right)\\right)\\beta_{s}\\ ,\\ \\forall i \\neq j = 1,\\dots,n\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Semi-dual damped Newton using direct solver without any preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi dual damped Newton\n",
    "print(  \" Semi dual damped Newton... \"   )\n",
    "print( \" Doing for (\",N[0], N[1],\"). \")\n",
    "rho = 0.95\n",
    "c = 0.5\n",
    "Semi_dual_dampedNewtonP = []    \n",
    "results_semi_dual_dampedNewton = []\n",
    "times_semi_dual_dampedNewton = []\n",
    "Hessians_semi_dual_dampedNewton = []\n",
    "#Cost matrix\n",
    "C = distmat( x, y )\n",
    "# a and b\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "for eps in epsilons:\n",
    "    K = np.exp( - C/eps )\n",
    "    print( \"For epsilon = \"+str(eps)+\":\" )   \n",
    "    f = a\n",
    "    print( \" |- Iterating\" )  \n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.semi_dual_dampedNewton_np( C,\n",
    "                                                            a,\n",
    "                                                            b,\n",
    "                                                            f,\n",
    "                                                            eps,\n",
    "                                                            rho,\n",
    "                                                            c,\n",
    "                                                            log_exp = \"True\" ) \n",
    "    out = Optimizer._update( max_iterations = 50 )\n",
    "    end = time.time()\n",
    "    if out != -1:\n",
    "        results_semi_dual_dampedNewton.append( out )\n",
    "        times_semi_dual_dampedNewton.append( end - start )\n",
    "        print( \" |- Computing P \" )\n",
    "        print( \"\" )\n",
    "        u_opt = np.exp( out['potential_f']/eps )\n",
    "        K = np.exp( - C/eps )\n",
    "        v_opt =  np.exp( out['potential_g']/eps )\n",
    "        P_opt = GetP( u_opt, K, v_opt )\n",
    "        Semi_dual_dampedNewtonP.append( P_opt )\n",
    "        print( \" |- Recording (unstabilized) Hessian \\n \" )\n",
    "        mat  = - eps * Optimizer.Hessian\n",
    "        diag = 1/np.sqrt( a )\n",
    "        mat = diag[:, None] * mat * diag[None,:]\n",
    "        Hessians_semi_dual_dampedNewton.append( mat )\n",
    "    else:\n",
    "        epsilons.remove( eps )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 12, 5 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" )\n",
    "for i in range(len(results_semi_dual_dampedNewton)):\n",
    "  error = np.asarray(results_semi_dual_dampedNewton[i]['error'])\n",
    "  plt.plot( error, label = 'Semi dual damped Newton for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2)\n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )\n",
    "plt.ylabel( \"Error in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.tight_layout()\n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_semi_dual_dampedNewton.pdf\", format = 'pdf' ) \n",
    "plt.show()\n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the final potentials of log-domain Sinkhorn and damped Newton in the semi-dual setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewtonSemiDual, gdampednewtonSemiDual = [], []\n",
    "for i in range(len(results_semi_dual_dampedNewton)):\n",
    "    fdampednewtonSemiDual.append( results_semi_dual_dampedNewton[i]['potential_f'] )\n",
    "    gdampednewtonSemiDual.append( results_semi_dual_dampedNewton[i]['potential_g'] )\n",
    "# end for\n",
    "unique_dampednewtonSemiDual= []\n",
    "for i in range(len(results_semi_dual_dampedNewton)):\n",
    "    unique_dampednewtonSemiDual.append( make_unique_potentials( fdampednewtonSemiDual[i], gdampednewtonSemiDual[i] ) ) \n",
    "# end for\n",
    "errors_f, errors_g = [], []\n",
    "for i in range(len(results_semi_dual_dampedNewton)):\n",
    "    print( f'''i : {i}''' )\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonSemiDual[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonSemiDual[i][1] )\n",
    "    errors_f.append( err_f )\n",
    "    errors_g.append( err_g )\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"Difference between potentials.\" )\n",
    "plt.plot( list(range(len(epsilons))), np.array(errors_f[::-1]) + np.array(errors_g[::-1]), label = 'difference for potentials ( f, g ) between log-domain Sinkhorn and damped Newton  in the semi dual setup', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_semi_dual_dampedNewton.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Semi-dual damped Newton with preconditioning\n",
    "Here we perform semi-dual damped Newton with preconditioning. Here we consider $t$ eigenvalues of the Hessian that we want to move to one and form the following preconditioning matrix using the corresponding eigenvectors,\n",
    "$$\n",
    "P = \\left(I_{n+m}-\\sum_{i-1}^{t}\\left(1 - \\frac{1}{\\sqrt{\\lambda_{i}}}\\right)y_{i}y_{i}^{T}\\right)\\ ,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "y_{i} \\in \\ker\\left(\\nabla^{2}_{f}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)-\\lambda_{i}I_{n}\\right),\\ \\forall i= 1,\\dots,k\\ ,\n",
    "$$\n",
    " are orthonormal.\n",
    "\n",
    "Now, at the $k^{th}$ iteration we solve the following equation:\n",
    "$$\n",
    "(P\\nabla^{2}Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)P)(Pp_{k})=P\\nabla Q^{semi}_{\\alpha, \\beta, \\varepsilon}(f)\\ ,\n",
    "$$\n",
    "using iterative inversion methods such as \"Conjugate gradient\" and \"GMRES\" to get the ascent direction $p_{k}$, following which we use the Armijo condition to obtain the ascent step size $\\alpha_{k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preconditioners( num_eigs, modified_Hessian, N, ansatz = True ):\n",
    "    # Diagonalize\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh( modified_Hessian )\n",
    "    sorting_indices = np.argsort( eigenvalues )\n",
    "    eigenvalues  = eigenvalues[ sorting_indices ]\n",
    "    eigenvectors = eigenvectors[ : , sorting_indices ]\n",
    "    # Form null vector\n",
    "    if not ansatz:\n",
    "        null_vector = eigenvectors[:, 0]\n",
    "    else:\n",
    "        null_vector = np.hstack( ( np.ones(N[0]) ) )\n",
    "        norm = np.sqrt( N[0])\n",
    "        null_vector = null_vector/norm\n",
    "    # Form other vectors\n",
    "    indices = []\n",
    "    for i in range(num_eigs):\n",
    "        indices.append( i + 1 )\n",
    "    # end for\n",
    "    precond_vectors = eigenvectors[:, indices ]\n",
    "    precond_vectors = []\n",
    "    for index in indices:\n",
    "        precond_vectors.append( eigenvectors[:,index] )\n",
    "    # end for\n",
    "    return null_vector, precond_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigs = 30\n",
    "null_vector, precond_vectors = build_preconditioners( num_eigs, Hessians_semi_dual_dampedNewton[-1], N, ansatz = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi dual damped Newton with preconditioning\n",
    "print(  \" Semi dual damped Newton with preconditioning... \"   )\n",
    "print( \" Doing for (\",N[0], N[1],\"). \")\n",
    "rho = 0.95\n",
    "c = 0.5\n",
    "reset_starting_point    = True  \n",
    "final_modified_Hessians = []\n",
    "Semi_dual_dampedNewton_with_preconditionerP          = []\n",
    "results_semi_dual_dampedNewton_with_preconditioner   = []\n",
    "times_semi_dual_dampedNewton_with_preconditioner     = []\n",
    "f, g = None, None\n",
    "# Cost matrix\n",
    "C = distmat( x, y )\n",
    "# a and b\n",
    "a = normalize( np.ones( N[0] ) )\n",
    "b = normalize( np.ones( N[1] ) )\n",
    "for eps in epsilons:\n",
    "    print( \"For epsilon = \"+str(eps)+\":\" )    \n",
    "    if (f is None):\n",
    "        f = a * 0\n",
    "    print( \" |- Iterating\" )  \n",
    "    start = time.time()\n",
    "    Optimizer = computational_OT.semi_dual_dampedNewton_with_preconditioning_np(    C,\n",
    "                                                                                    a,\n",
    "                                                                                    b,\n",
    "                                                                                    f,\n",
    "                                                                                    eps,\n",
    "                                                                                    rho,\n",
    "                                                                                    c,\n",
    "                                                                                    null_vector,\n",
    "                                                                                    precond_vectors[:],\n",
    "                                                                                    exp_log = \"True\" )\n",
    "    out = Optimizer._update(    max_iterations = 50,\n",
    "                                iterative_inversion = 30,\n",
    "                                version = None,\n",
    "                                debug = False, \n",
    "                                optType = 'cg' )\n",
    "    results_semi_dual_dampedNewton_with_preconditioner.append( out )\n",
    "    end = time.time()\n",
    "    times_semi_dual_dampedNewton_with_preconditioner.append( end - start )\n",
    "    print( \" |- Computing P\" )\n",
    "    print( \"\" )\n",
    "    u_opt = np.exp( out['potential_f']/eps )\n",
    "    K = np.exp( - C/eps )\n",
    "    v_opt =  np.exp( out['potential_g']/eps )\n",
    "    P_opt = GetP( u_opt, K, v_opt )\n",
    "    Semi_dual_dampedNewton_with_preconditionerP.append( P_opt )\n",
    "    if not reset_starting_point:\n",
    "        f = Optimizer.x[:a.shape[0]]\n",
    "        g = Optimizer.x[a.shape[0]:]\n",
    "    final_modified_Hessians.append( Optimizer.modified_Hessian )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) ) \n",
    "plt.title( \"$$\" ) \n",
    "plt.title( \"$||P1 -a||_1+||P1 -b||_1$\" ) \n",
    "for i in range(len(results_semi_dual_dampedNewton_with_preconditioner)): \n",
    "  error = np.asarray(results_semi_dual_dampedNewton_with_preconditioner[i]['error'] ) \n",
    "  plt.plot( error, label = 'Semi dual damped Newton for $\\epsilon = $'+ str(epsilons[i]), linewidth = 2 ) \n",
    "# end for\n",
    "plt.xlabel( \"Number of iterations\" )  \n",
    "plt.ylabel( \"Error in log-scale\" )  \n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' ) \n",
    "plt.savefig( \"../Images/Correctness_images/Error_plot_semi_dual_dampedNewton_with_preconditioning.pdf\", format = 'pdf' ) \n",
    "plt.show() \n",
    "print( \"\\n Error plots can increase! The error is not the objective function!\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the final potentials of log-domain Sinkhorn and damped Newton with preconditioning in the semi-dual setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdampednewtonSemiDualwithprecond, gdampednewtonSemiDuawithprecond = [], []\n",
    "for i in range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    fdampednewtonSemiDualwithprecond.append(results_semi_dual_dampedNewton_with_preconditioner[i]['potential_f'])\n",
    "    gdampednewtonSemiDuawithprecond.append(results_semi_dual_dampedNewton_with_preconditioner[i]['potential_g'])\n",
    "# end for\n",
    "unique_dampednewtonSemiDualwithprecond = []\n",
    "for i in range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    unique_dampednewtonSemiDualwithprecond.append(make_unique_potentials( fdampednewtonSemiDualwithprecond[i], gdampednewtonSemiDuawithprecond[i]))\n",
    "# end for\n",
    "errors_f,errors_g = [], []\n",
    "for i in range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    print( f'''i : {i}''')\n",
    "    err_f = np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonSemiDualwithprecond[i][0] )\n",
    "    err_g = np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonSemiDualwithprecond[i][1] )\n",
    "    errors_f.append(err_f)\n",
    "    errors_g.append(err_g)\n",
    "    print( \"norm of err_f: \", err_f )\n",
    "    print( \"norm of err_g: \", err_g )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the final potentials of semi-dual damped Newton and semi-dual damped Newton with preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality checks\n",
    "print( \"For semi-dual damped Newton with and without precodnitioning\" )\n",
    "for i in range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    print( f'''i : {i}''' )\n",
    "    # Sums of potentials f_i + g_j\n",
    "    sum_dampedNewtonSemiDual    = fdampednewtonSemiDual[i][:,None] + gdampednewtonSemiDual[i][None,:]\n",
    "    sum_dampedNewtonSemiDualprecond = fdampednewtonSemiDualwithprecond[i][:,None] + gdampednewtonSemiDuawithprecond[i][None,:]\n",
    "    sum_dampedNewtonSemiDual    = sum_dampedNewtonSemiDual.squeeze()\n",
    "    sum_dampedNewtonSemiDualprecond = sum_dampedNewtonSemiDualprecond.squeeze()\n",
    "    error = np.linalg.norm( sum_dampedNewtonSemiDual - sum_dampedNewtonSemiDualprecond, ord = np.inf )\n",
    "    print( \"Error of sums of potentials : \", error )\n",
    "    print( \"\" )\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"Difference between potentials.\" )\n",
    "plt.plot( list(range(len(epsilons))), np.array(errors_f[::-1]) + np.array(errors_g[::-1]), label = 'difference for potentials ( f, g ) between log-domain Sinkhorn  and damped Newton with preconditioning in the semi dual setup', linewidth = 2, marker= 'o' )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = \"upper right\" )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_semi_dual_dampedNewton_with_preconditioning.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison plot for comparing the Kantorovich potentials of the various algortihms used above against the ground truth: log-domain Sinkhorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 20, 7 ) )\n",
    "plt.title( \"$$\" )\n",
    "plt.title( \"Difference between potentials with and without regularization.\" )\n",
    "# Plot for log-domain Sinkhorn vs Sinkhorn\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_Sinkhorn)):\n",
    "    difference_f.append( np.linalg.norm( unique_logSK[i][0] - unique_SK[i][0] ) )\n",
    "# end for\n",
    "for i in  range(len(results_Sinkhorn)):\n",
    "    difference_g.append( np.linalg.norm( unique_logSK[i][1] - unique_SK[i][1] ) ) \n",
    "# end for\n",
    "plt.plot( list(range(len(epsilons))), np.array(difference_f[::-1]) + np.array(difference_g[::-1]),  label = 'difference for potential ( f, g ) between log-domain Sinkhorn and Sinkhorn', linewidth = 2, marker = 'o' )\n",
    "\n",
    "# Plot for log-domain Sinkhorn vs damped Newton\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_dampedNewton)):\n",
    "    difference_f.append( np.linalg.norm( unique_logSK[i][0] - unique_dampednewton[i][0] ) )\n",
    "# end for\n",
    "for i in  range(len(results_dampedNewton)):\n",
    "    difference_g.append( np.linalg.norm( unique_logSK[i][1] - unique_dampednewton[i][1] ) )\n",
    "# end for\n",
    "plt.plot( list(range(len(epsilons))), np.array(difference_f[::-1]) + np.array(difference_g[::-1]), label = 'difference for potential ( f, g ) between log-domain Sinkhorn and  damped Newton', linewidth = 2, marker = 'o' )\n",
    "\n",
    "# Plot for log-domain Sinkhorn vs damped Newton with preconditioning\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_dampedNewtonwithprecond)):\n",
    "    difference_f.append( np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonwithprecond[i][0] ) )\n",
    "# end for\n",
    "for i in  range(len(results_dampedNewtonwithprecond)):\n",
    "    difference_g.append( np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonwithprecond[i][1] ) )\n",
    "# end for\n",
    "plt.plot( list(range(len(epsilons))), np.array(difference_f[::-1]) + np.array(difference_g[::-1]), label = 'difference for potential ( f, g ) between log-domain Sinkhorn and damped Newton with preconditioning', linewidth = 2, marker = 'o' )\n",
    "\n",
    "# Plot for log-domain Sinkhorn vs semi-dual damped Newton \n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_semi_dual_dampedNewton)):\n",
    "    difference_f.append( np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonSemiDual[i][0] ) )\n",
    "# end for\n",
    "for i in  range(len(results_semi_dual_dampedNewton)):\n",
    "    difference_g.append( np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonSemiDual[i][1] ) )\n",
    "# end for\n",
    "plt.plot( list(range(len(epsilons))), np.array(difference_f[::-1]) + np.array(difference_g[::-1]), label = 'difference for potential ( f, g ) between log-domain Sinkhorn and  semi-dual damped Newton', linewidth = 2, marker = 'o' )\n",
    "\n",
    "# Plot for log-domain Sinkhorn vs semi-dual damped Newton with preconditioning\n",
    "difference_f = []\n",
    "difference_g = []\n",
    "for i in  range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    difference_f.append( np.linalg.norm( unique_logSK[i][0] - unique_dampednewtonSemiDualwithprecond[i][0] ) )\n",
    "# end for\n",
    "for i in  range(len(results_semi_dual_dampedNewton_with_preconditioner)):\n",
    "    difference_g.append( np.linalg.norm( unique_logSK[i][1] - unique_dampednewtonSemiDualwithprecond[i][1] ) )\n",
    "# end for\n",
    "plt.plot( list(range(len(epsilons))), np.array(difference_f[::-1]) + np.array(difference_g[::-1]), label = 'difference for potential ( f, g ) between log-domain Sinkhorn and  semi-dual damped Newton with preconditioning', linewidth = 2, marker = 'o' )\n",
    "plt.xlabel( \"$\\epsilon$\" )\n",
    "plt.ylabel( \"difference in log-scale\" )\n",
    "plt.legend( loc = 'upper right' )\n",
    "plt.yscale( 'log' )\n",
    "plt.xticks( list(range(len(epsilons))), epsilons[::-1] )\n",
    "plt.savefig( \"../Images/Correctness_images/Correctness_comparison_plot_all_algorithms.pdf\", format = 'pdf' ) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_computational-OT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
