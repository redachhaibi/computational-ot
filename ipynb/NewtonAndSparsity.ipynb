{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropic Regularization of Optimal Transport\n",
    "============================================\n",
    "\n",
    "*Important:* Please read the [installation page](http://gpeyre.github.io/numerical-tours/installation_matlab/) for details about how to install the toolboxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This numerical tours exposes the general methodology of regularizing the\n",
    "optimal transport (OT) linear program using entropy. This allows to\n",
    "derive fast computation algorithm based on iterative projections\n",
    "according to a Kulback-Leiber divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "import pylab as pyl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To compute distance matrix\"\"\"\n",
    "def distmat(x,y):\n",
    "    \n",
    "    return np.sum(x**2,0)[:,None] + np.sum(y**2,0)[None,:] - 2*x.transpose().dot(y)\n",
    "\n",
    "\"\"\"To Normalise a vector\"\"\"\n",
    "normalize = lambda a: a/np.sum(a)\n",
    "\n",
    "\"\"\"To Compute P\"\"\"\n",
    "def GetP(u,K,v):\n",
    "    return u*K*(v.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropic Regularization of Optimal Transport\n",
    "--------------------------------------------\n",
    "We consider the following discrete regularized transport\n",
    "$$  W_\\varepsilon(a,b) := \\umin{P \\in U(a,b)} \\dotp{C}{P} - \\varepsilon E(P).  $$\n",
    "where the polytope of coupling is defined as\n",
    "$$ U(a,b) := \\enscond{P \\in (\\mathbb{R}^+)^{n \\times m}}{ P \\ones_m = a, P^\\top \\ones_n = b },  $$\n",
    "where $\\ones_n := (1,\\ldots,1)^\\top \\in \\mathbb{R}^n $,\n",
    "and for $P \\in \\mathbb{R}_+^{n \\times m}$, we define its entropy as\n",
    "$$ E(P) := -\\sum_{i,j} P_{i,j} ( \\log(P_{i,j}) - 1). $$\n",
    "\n",
    "\n",
    "When $\\varepsilon=0$ one recovers the classical (discrete) optimal transport.\n",
    "We refer to the monograph [Villani](#biblio) for more details about OT.\n",
    "The idea of regularizing transport to allows for faster computation is\n",
    "introduced in [Cuturi](#biblio).\n",
    "\n",
    "\n",
    "Here the matrix $C \\in (\\mathbb{R}^+)^{n \\times m} $ defines the ground cost, i.e.\n",
    "$C_{i,j}$ is the cost of moving mass from a bin indexed by $i$ to a bin indexed by $j$.\n",
    "\n",
    "\n",
    "The regularized transportation problem can be re-written as a projection\n",
    "$$ W_\\varepsilon(a,b) = \\varepsilon \\umin{P \\in U(a,b)} \\KLdiv{P}{K}\n",
    "\t\\qwhereq\n",
    "\tK_{i,j} := e^{ -\\frac{C_{i,j}}{\\varepsilon} }  $$\n",
    "of the Gibbs kernel $K$ according to the Kullback-Leibler divergence.\n",
    "The Kullback-Leibler divergence between $P, K \\in \\mathbb{R}_+^{n \\times m}$ is\n",
    "$$ \\KLdiv{P}{K} := \\sum_{i,j} P_{i,j} \\pa{ \\log\\pa{ \\frac{P_{i,j}}{K_{i,j}} } - 1}. $$\n",
    "\n",
    "\n",
    "This interpretation of regularized transport as a KL projection and its numerical\n",
    "applications are detailed in [BenamouEtAl](#biblio).\n",
    "\n",
    "\n",
    "Given a convex set $\\Cc \\subset \\mathbb{R}^N$, the projection according to the Kullback-Leiber divergence is defined as\n",
    "$$ \\KLproj_\\Cc(\\xi) = \\uargmin{ \\pi \\in \\Cc } \\KLdiv{\\pi}{\\xi}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction to Sinkhorn's Algorithm\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "A fundamental remark is that the optimality condition of the entropic regularized problem shows that the optimal coupling $P_\\varepsilon$ necessarily has the form \n",
    "$$P_\\varepsilon = \\Delta{u} K \\Delta{v}$$\n",
    "where the Gibbs kernel is defined as\n",
    "$$K := e^{-\\frac{C}{\\varepsilon}}.$$\n",
    "\n",
    "One thus needs to find two positive scaling vectors $u \\in \\mathbb{R}_+^n$ and $v \\in \\mathbb{R}_+^m$ such that the two following equality holds\n",
    "$$P \\ones  = u \\odot (K v) = a \n",
    "\\textrm{ and }\n",
    "P^\\top \\ones  = v \\odot (K^\\top u) = b.$$\n",
    "\n",
    "Sinkhorn's algorithm alternate between the resolution of these two equations, and reads\n",
    "$$u \\longleftarrow \\frac{a}{K v} \n",
    "\\textrm{ and }\n",
    "v \\longleftarrow \\frac{b}{K^\\top u}.$$\n",
    "This algorithm was shown to converge to a solution of the entropic regularized problem by [Sinkhorn](#biblio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transport Between Point Clouds\n",
    "------------------------------\n",
    "We first test the method for two input measures that are uniform measures\n",
    "(i.e. constant histograms) supported on two point clouds\n",
    "(that do not necessarily have the same size).\n",
    "\n",
    "\n",
    "We thus first load two points clouds $x=(x_i)_{i=1}^{n}, y=(y_i)_{i=1}^{m}, $\n",
    "where $x_i, y_i \\in \\mathbb{R}^2$.\n",
    "\n",
    "\n",
    "Number of points in each cloud, $N=(n,m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100)\n",
    "Y = np.random.rand(100)\n",
    "s = np.argsort(X)\n",
    "print(\"Indices de tri: \\n\", s)\n",
    "print(\"Echantillon triÃ©:\\n\", X[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [400,400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension of the clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud $x$, of $n$ points inside a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2,N[0])-.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud $y$, of $m$ points inside an anulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2*np.pi*np.random.rand(1,N[1])\n",
    "r = .8 + .2*np.random.rand(1,N[1])\n",
    "y = np.vstack((np.cos(theta)*r,np.sin(theta)*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut for displaying point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotp = lambda x,col: plt.scatter(x[0,:], x[1,:], s=200, edgecolors=\"k\", c=col, linewidths=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of the two clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plotp(x, 'b')\n",
    "plotp(y, 'r')\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.xlim(np.min(y[0,:])-.1,np.max(y[0,:])+.1)\n",
    "plt.ylim(np.min(y[1,:])-.1,np.max(y[1,:])+.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost matrix $C_{i,j} = \\|x_i-y_j\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C = distmat(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target histograms $(a,b)$, here uniform histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(N[0])/N[0]\n",
    "b = np.ones(N[1])/N[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import computational_OT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs Kernel $K$.\n",
    "\n",
    "Initialization of $v=\\ones_{m}$ ($u$ does not need to be\n",
    "initialized).\n",
    "\n",
    "Regularization strength $\\varepsilon>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__\n",
    "\n",
    "Implement Sinkhorn algorithm.\n",
    "Display the evolution of the constraints satisfaction errors\n",
    "$$ \\| P \\mathbb{1} - a \\|_1 \n",
    "\\textrm{ and }\n",
    "\\| P^\\top \\mathbb{1} - b \\| $$\n",
    "(you need to think about how to compute these residuals from $(u,v)$ alone).\n",
    "isplay the violation of constraint error in log-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = .01\n",
    "K = np.exp(-C/epsilon)\n",
    "u=np.ones(N[0])\n",
    "v = np.ones(N[1])\n",
    "\n",
    "\n",
    "\n",
    "SOptimizer=computational_OT.Sinkhorn(K,a,b,u,v,epsilon)\n",
    "out=SOptimizer._update(maxiter=1000)\n",
    "#\n",
    "# Plot\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.subplot(2,1,1),\n",
    "plt.title(\"$||P1 -a||_1$\")\n",
    "plt.plot( np.asarray(out[2]), linewidth = 2)\n",
    "plt.yscale( 'log')\n",
    "plt.ylabel(\"Error in log scale\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.legend([\"Sample size: \"+str(i)+\" and Epsilon=\"+str(epsilon) for i in N],loc=\"upper right\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"$||P^T 1 -b||_1$\")\n",
    "plt.plot( np.asarray(out[3]), linewidth = 2)\n",
    "plt.yscale( 'log')\n",
    "plt.ylabel(\"Error in log scale\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.legend([\"Sample size: \"+str(i)+\" and Epsilon=\"+str(epsilon) for i in N],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Second order iteration: The Newton-Raphson scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilon = .01\n",
    "K = np.exp(-C/epsilon)\n",
    "u=np.ones(N[0])\n",
    "v = np.ones(N[1])\n",
    "\n",
    "\n",
    "SOptimizer=computational_OT.Sinkhorn(K,a,b,u,v,epsilon)\n",
    "outS=SOptimizer._update(maxiter=100)\n",
    "\n",
    "\n",
    "X = np.hstack( (outS[0],outS[1]) )\n",
    "X = epsilon*np.log(X)\n",
    "NOptimizer= computational_OT.NewtonRaphson(X,K,a,b,epsilon)\n",
    "outN=NOptimizer._update(maxiter=10,debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \" errors:\", outN[0],outN[1])\n",
    "print( np.array(outN[0]) + np.array(outN[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.subplot(2,1,1),\n",
    "plt.title(\"$||P1 -a||_1$\")\n",
    "plt.plot( np.asarray(out[2]),label='Sinkhorn for $\\varepsilon=$' + str(epsilon), linewidth = 2)\n",
    "plt.plot( np.asarray(outS[2]+outN[0]),label='Hybrid method for $\\varepsilon=$'+ str(epsilon), linewidth = 2)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Error in log-scale\")\n",
    "plt.legend()\n",
    "plt.yscale( 'log')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"$||P^T 1 -b||_1$\")\n",
    "plt.plot( np.asarray(out[3]), label='Sinkhorn for $\\varepsilon=$' + str(epsilon), linewidth = 2)\n",
    "plt.plot( np.asarray(outS[3]+outN[1]),label='Hybrid method for $\\varepsilon=$'+ str(epsilon), linewidth = 2)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Error in log-scale\")\n",
    "plt.legend()\n",
    "plt.yscale( 'log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Explore the sparsity of the matrices involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the final matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.dot(np.dot(np.diag(outS[0]),K),np.diag(outS[1]))\n",
    "Q1 = np.sort(P, axis=0)\n",
    "Q2 = np.sort(P, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( P.flatten(), 20)\n",
    "plt.xscale( 'log')\n",
    "plt.yscale( 'log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(P);\n",
    "plt.figure()\n",
    "plt.imshow(Q1);\n",
    "plt.figure()\n",
    "plt.imshow(Q2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_xx = np.dot(P, P.T)\n",
    "P_yy = np.dot(P.T, P)\n",
    "plt.figure()\n",
    "plt.imshow( P_xx );\n",
    "plt.figure()\n",
    "plt.imshow( P_yy );\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist( P_xx.flatten(), 20, cumulative=True, density=True)\n",
    "plt.xscale( 'log')\n",
    "plt.ylim( (0,1) )\n",
    "plt.show()\n",
    "plt.hist( P_yy.flatten(), 20, cumulative=False)\n",
    "plt.xscale( 'log')\n",
    "plt.yscale( 'log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuthill-Mckee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import reverse_cuthill_mckee\n",
    "\n",
    "def invert_permutation(p):\n",
    "    \"\"\"Return an array s with which np.array_equal(arr[p][s], arr) is True.\n",
    "    The array_like argument p must be some permutation of 0, 1, ..., len(p)-1.\n",
    "    \"\"\"\n",
    "    p = np.asanyarray(p) # in case p is a tuple, etc.\n",
    "    s = np.empty_like(p)\n",
    "    s[p] = np.arange(p.size)\n",
    "    return s\n",
    "\n",
    "cut_offx=1e-7\n",
    "cut_offy=1e-7\n",
    "P_xx_   = P_xx*( P_xx > cut_offx)\n",
    "P_xx_csr = csr_matrix(P_xx_)\n",
    "perm_x = reverse_cuthill_mckee(P_xx_csr)\n",
    "invp_x = invert_permutation(perm_x)\n",
    "\n",
    "P_yy_ = P_yy*( P_yy > cut_offy)\n",
    "P_yy_csr = csr_matrix(P_yy_)\n",
    "perm_y = reverse_cuthill_mckee(P_yy_csr)\n",
    "invp_y = invert_permutation(perm_y)\n",
    "\n",
    "mesh = np.meshgrid( perm_x, perm_y )\n",
    "P_ = P[mesh]\n",
    "mesh = np.meshgrid( perm_x, perm_x )\n",
    "P_xx_ = P_xx[mesh]\n",
    "mesh = np.meshgrid( perm_y, perm_y )\n",
    "P_yy_ = P_yy[mesh]\n",
    "\n",
    "size=10\n",
    "plt.figure( figsize=(size,size))\n",
    "plt.imshow( P_xx_ );\n",
    "plt.figure( figsize=(size,size))\n",
    "plt.imshow( P_yy_ );\n",
    "plt.figure( figsize=(size,size))\n",
    "plt.imshow( P_ );\n",
    "plt.figure( figsize=(size,size))\n",
    "plt.imshow( P_xx );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD and low rank approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v = np.linalg.svd(P)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Decay of singular values\")\n",
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering from Euclidean distance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Shape:\")\n",
    "print(x.shape)\n",
    "\n",
    "Z = linkage(x.T, 'centroid')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "\n",
    "W = np.array( Z, dtype=int)\n",
    "#print(W)\n",
    "\n",
    "count = x.shape[1]\n",
    "clusters = [ [i] for i in range(count) ]\n",
    "for merge in W:\n",
    "    i,j,k,l = merge\n",
    "    new_cluster = clusters[i] + clusters[j]\n",
    "    clusters.append( new_cluster )\n",
    "    \n",
    "p1 = np.array( clusters[-1] )\n",
    "print(p1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Shape:\")\n",
    "print(x.shape)\n",
    "\n",
    "Z = linkage(y.T, 'centroid')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "\n",
    "W = np.array( Z, dtype=int)\n",
    "#print(W)\n",
    "\n",
    "count = x.shape[1]\n",
    "clusters = [ [i] for i in range(count) ]\n",
    "for merge in W:\n",
    "    i,j,k,l = merge\n",
    "    new_cluster = clusters[i] + clusters[j]\n",
    "    clusters.append( new_cluster )\n",
    "    \n",
    "p2 = np.array( clusters[-1] )\n",
    "print(p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_P = np.empty_like(P)\n",
    "clustered_P[np.arange(p1.size)] = P[p1]\n",
    "clustered_P = clustered_P.T\n",
    "clustered_P[np.arange(p2.size)] = clustered_P[p2]\n",
    "clustered_P = clustered_P.T\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(clustered_P);\n",
    "\n",
    "clustered_P_xx = np.dot( clustered_P, clustered_P.T )\n",
    "clustered_P_yy = np.dot( clustered_P.T, clustered_P )\n",
    "plt.figure()\n",
    "plt.imshow( clustered_P_xx );\n",
    "plt.figure()\n",
    "plt.imshow( clustered_P_yy );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering from the optimal transport ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Shape:\")\n",
    "print( P_xx.shape)\n",
    "\n",
    "Z = linkage(P_xx, 'complete')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "\n",
    "W = np.array( Z, dtype=int)\n",
    "\n",
    "count = x.shape[1]\n",
    "clusters = [ [i] for i in range(count) ]\n",
    "for merge in W:\n",
    "    i,j,k,l = merge\n",
    "    new_cluster = clusters[i] + clusters[j]\n",
    "    clusters.append( new_cluster )\n",
    "    \n",
    "p1 = np.array( clusters[-1] )\n",
    "print(p1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Shape:\")\n",
    "print(y.shape)\n",
    "\n",
    "Z = linkage(P_yy, 'complete')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "\n",
    "W = np.array( Z, dtype=int)\n",
    "#print(W)\n",
    "\n",
    "count = x.shape[1]\n",
    "clusters = [ [i] for i in range(count) ]\n",
    "for merge in W:\n",
    "    i,j,k,l = merge\n",
    "    new_cluster = clusters[i] + clusters[j]\n",
    "    clusters.append( new_cluster )\n",
    "    \n",
    "p2 = np.array( clusters[-1] )\n",
    "print(p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_P = np.empty_like(P)\n",
    "clustered_P[np.arange(p1.size)] = P[p1]\n",
    "clustered_P = clustered_P.T\n",
    "clustered_P[np.arange(p2.size)] = clustered_P[p2]\n",
    "clustered_P = clustered_P.T\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(clustered_P);\n",
    "\n",
    "clustered_P_xx = np.dot( clustered_P, clustered_P.T )\n",
    "clustered_P_yy = np.dot( clustered_P.T, clustered_P )\n",
    "plt.figure()\n",
    "plt.imshow( clustered_P_xx );\n",
    "plt.figure()\n",
    "plt.imshow( clustered_P_yy );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv_computational-OT': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b36e04c518f81ce2975e9f25da352c3435a6c877efce8156a6920cdb02a8e6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
